# Story 2.2: Stream Camera Frames via WebSocket

## Description
As a developer, I want to capture frames from the live camera feed and send them through the active WebSocket connection, so that the backend can perform facial recognition.

## Acceptance Criteria
1. When the app is in the "streaming" state, it begins capturing and processing frames.
2. Frames are formatted as required by the backend API (e.g., Base64 encoded).
3. Frames are sent via the WebSocket at a regular interval.
4. Frame streaming is handled efficiently to avoid performance issues.

## Technical Details
- **Frame Capture:**
  - Implement frame capture from camera controller
  - Convert frames to appropriate format (Base64)
  - Implement frame rate control
  - Handle frame processing errors

- **WebSocket Integration:**
  - Send frames through WebSocket connection
  - Implement frame queue to prevent blocking
  - Handle WebSocket send errors
  - Monitor frame sending performance

- **Performance Optimization:**
  - Implement frame skipping if processing is slow
  - Add frame rate limiting
  - Monitor memory usage
  - Handle device performance variations

- **State Management:**
  - Add new states to `CheckInState`:
    - `StreamingFrames`
    - `FrameProcessingError`
    - `StreamingPaused`

## Definition of Done
- [ ] Frames are captured from camera
- [ ] Frames are properly encoded and sent
- [ ] Frame rate is controlled to prevent performance issues
- [ ] Frame processing errors are handled
- [ ] Memory usage is optimized
- [ ] All states are properly handled in the UI
- [ ] Code follows the project's linting rules 