# Story 2.2: Frame Streaming & Processing

## Status: Ready for TDD Development

## Story

- As a **user**
- I want **the app to capture camera frames and stream them to the backend for face recognition, then process the responses**
- so that **I can see real-time face detection results and status updates**

## Pre-Development Test Planning

### **Test Strategy**
- [ ] **Test Approach Defined**: Unit tests for frame processing, Widget tests for real-time UI updates, Integration tests for streaming workflow
- [ ] **Mock Strategy**: Mock camera frames, Mock WebSocket responses, Mock timing for performance testing
- [ ] **Test Data**: Sample camera frames, Mock face detection responses, Performance benchmarks
- [ ] **Test Environment**: Frame processing simulation with controlled timing and mock backend responses

### **TDD Development Plan**
```markdown
1. **RED Phase Tests**: List of failing tests to write first
   - [ ] Test 1: Frame capture and base64 conversion tests
   - [ ] Test 2: Frame streaming and WebSocket integration tests
   - [ ] Test 3: Response parsing and face detection processing tests
   - [ ] Test 4: Real-time UI updates and performance tests
   
2. **GREEN Phase Implementation**: Minimal code to pass tests
   - [ ] Implementation 1: Basic frame capture and conversion service
   - [ ] Implementation 2: Frame streaming with WebSocket integration
   - [ ] Implementation 3: Response parsing and face detection data management
   - [ ] Implementation 4: Real-time UI updates and BLoC integration
   
3. **REFACTOR Phase**: Code quality improvements
   - [ ] Refactor 1: Optimize frame processing performance and memory usage
   - [ ] Refactor 2: Improve streaming reliability and error handling
   - [ ] Refactor 3: Enhance response processing and data management
   - [ ] Refactor 4: Clean up real-time UI updates and state management
```

## Acceptance Criteria (TDD Format)

### AC1: Frame Capture & Conversion
**Given** camera is initialized and streaming  
**When** frame capture is requested at 30 FPS  
**Then** camera frames should be captured consistently  
**And** frames should be converted to base64 format efficiently

**Test Scenarios:**
- [ ] **Happy Path**: Frames captured at 30 FPS and converted to base64 successfully
- [ ] **Error Case**: Frame capture failures handled gracefully with error recovery
- [ ] **Edge Case**: Performance optimization maintains frame rate under load

**Test Implementation Required:**
- [ ] **Unit Test**: Frame capture timing and base64 conversion accuracy
- [ ] **Widget Test**: Frame processing status display in UI
- [ ] **Integration Test**: End-to-end frame capture and conversion workflow

### AC2: Frame Streaming & WebSocket Integration
**Given** WebSocket connection is established  
**When** frame streaming is started  
**Then** base64 frames should be sent to backend consistently  
**And** streaming state should be managed through BLoC

**Test Scenarios:**
- [ ] **Happy Path**: Frames stream successfully with proper state management
- [ ] **Error Case**: Streaming failures trigger appropriate error handling and recovery
- [ ] **Edge Case**: Network interruptions handled with streaming pause/resume

**Test Implementation Required:**
- [ ] **Unit Test**: Frame streaming logic and WebSocket message sending
- [ ] **Widget Test**: Streaming status indicators and controls in UI
- [ ] **Integration Test**: Complete frame streaming workflow with state management

### AC3: Response Processing & Face Detection
**Given** frames are being streamed to backend  
**When** face detection responses are received  
**Then** responses should be parsed correctly  
**And** face detection results should be stored in BLoC state

**Test Scenarios:**
- [ ] **Happy Path**: Face detection responses parsed and processed correctly
- [ ] **Error Case**: Malformed responses handled with appropriate error states
- [ ] **Edge Case**: Multiple faces and complex detection scenarios handled properly

**Test Implementation Required:**
- [ ] **Unit Test**: Response parsing logic and face detection data management
- [ ] **Widget Test**: Face detection results display in UI
- [ ] **Integration Test**: End-to-end response processing and state updates

### AC4: Real-time UI Updates & Performance
**Given** face detection is active  
**When** detection results are processed  
**Then** UI should update in real-time  
**And** performance should meet real-time requirements

**Test Scenarios:**
- [ ] **Happy Path**: UI updates smoothly with face detection results
- [ ] **Error Case**: UI performance maintained under high processing load
- [ ] **Edge Case**: Multiple rapid detection updates handled efficiently

**Test Implementation Required:**
- [ ] **Unit Test**: Real-time update logic and performance optimization
- [ ] **Widget Test**: UI responsiveness and real-time display updates
- [ ] **Integration Test**: Complete real-time face detection workflow

## Story Estimation

**Story Points**: 24 SP (increased from 18 SP for TDD)  
**Complexity**: Very High - Real-time frame processing + Network streaming + Response handling + BLoC integration + Performance optimization + TDD implementation  
**Risk Level**: High - Performance requirements, real-time processing, network bandwidth, state synchronization, complex testing scenarios  
**TDD Effort**: 6 SP additional for comprehensive real-time testing, performance testing, and complex mock setups  
**Estimation Method**: Combined estimation from frame streaming and response processing + TDD overhead + performance testing complexity  
**Reference Stories**: Real-time streaming implementations + TDD experience from previous stories  
**Assumptions**: Camera service ready (Story 1.2), WebSocket service ready (Story 2.1), backend response format documented, TDD tooling configured for performance testing

## Tasks / Subtasks

### **Phase 1: TDD Pre-Development**
- [ ] **Task 1**: Frame Streaming Test Strategy & Mock Setup
  - [ ] Design real-time testing strategy with frame mocking
  - [ ] Setup mock camera frames and timing simulation
  - [ ] Prepare mock WebSocket responses for face detection
  - [ ] Create performance benchmarks and test fixtures
  - [ ] Document frame streaming testing approach and limitations

### **Phase 2: TDD Development (RED-GREEN-REFACTOR)**

#### **Task 2: Frame Capture & Conversion - TDD Cycle**
**ðŸ”´ RED Phase:**
- [ ] Write failing tests for 30 FPS frame capture
- [ ] Write failing tests for base64 conversion accuracy
- [ ] Write failing tests for frame processing performance
- [ ] Write failing tests for memory management during processing

**ðŸŸ¢ GREEN Phase:**
- [ ] Create frame capture service with 30 FPS timing
- [ ] Implement camera frame to base64 conversion
- [ ] Add frame capture error handling and validation
- [ ] Optimize conversion performance for real-time processing

**ðŸ”µ REFACTOR Phase:**
- [ ] Optimize frame capture timing and synchronization
- [ ] Improve base64 conversion performance and memory usage
- [ ] Enhance error handling and recovery mechanisms
- [ ] Clean up frame processing architecture

#### **Task 3: Frame Streaming Service - TDD Cycle**
**ðŸ”´ RED Phase:**
- [ ] Write failing tests for frame streaming to WebSocket
- [ ] Write failing tests for streaming state management
- [ ] Write failing tests for streaming start/stop functionality
- [ ] Write failing tests for streaming error handling

**ðŸŸ¢ GREEN Phase:**
- [ ] Create frame streaming service using WebSocket service
- [ ] Implement base64 frame transmission logic
- [ ] Add streaming start/stop functionality with state tracking
- [ ] Handle streaming errors and connection failures

**ðŸ”µ REFACTOR Phase:**
- [ ] Optimize streaming performance and reliability
- [ ] Improve streaming state management and transitions
- [ ] Enhance error handling and recovery logic
- [ ] Clean up streaming service architecture

#### **Task 4: Response Processing & Parsing - TDD Cycle**
**ðŸ”´ RED Phase:**
- [ ] Write failing tests for backend response parsing
- [ ] Write failing tests for face detection data extraction
- [ ] Write failing tests for response validation and error handling
- [ ] Write failing tests for multiple faces processing

**ðŸŸ¢ GREEN Phase:**
- [ ] Implement backend response parsing (JSON format)
- [ ] Create face detection data models and storage
- [ ] Add response validation and error handling
- [ ] Handle frameResult type and faces array structure

**ðŸ”µ REFACTOR Phase:**
- [ ] Optimize response parsing performance
- [ ] Improve face detection data management
- [ ] Enhance response validation and error handling
- [ ] Clean up response processing architecture

#### **Task 5: BLoC Streaming Integration - TDD Cycle**
**ðŸ”´ RED Phase:**
- [ ] Write failing tests for streaming state integration
- [ ] Write failing tests for face detection state management
- [ ] Write failing tests for streaming event handling
- [ ] Write failing tests for real-time state updates

**ðŸŸ¢ GREEN Phase:**
- [ ] Extend CheckInState for streaming and face detection
- [ ] Add streaming and detection events to CheckInEvent
- [ ] Implement streaming state transitions in CheckInBloc
- [ ] Create face detection result â†’ BLoC state mapping

**ðŸ”µ REFACTOR Phase:**
- [ ] Optimize BLoC streaming state management
- [ ] Improve event handling and state transitions
- [ ] Enhance real-time state update performance
- [ ] Clean up streaming BLoC integration

#### **Task 6: Real-time UI Integration - TDD Cycle**
**ðŸ”´ RED Phase:**
- [ ] Write failing tests for real-time UI updates
- [ ] Write failing tests for face detection status display
- [ ] Write failing tests for UI performance under load
- [ ] Write failing tests for multiple face detection UI

**ðŸŸ¢ GREEN Phase:**
- [ ] Connect face detection results to UI updates
- [ ] Implement real-time face detection status display
- [ ] Add visual feedback for face detection confidence
- [ ] Handle UI updates for multiple faces

**ðŸ”µ REFACTOR Phase:**
- [ ] Optimize real-time UI update performance
- [ ] Improve visual feedback and user experience
- [ ] Enhance UI responsiveness and smoothness
- [ ] Clean up real-time UI integration

### **Phase 3: TDD Comprehensive Testing**
- [ ] **Task 7**: Performance & Integration Testing
  - [ ] Add performance tests for 30 FPS frame processing
  - [ ] Add integration tests for complete streaming workflow
  - [ ] Add stress tests for continuous streaming operation
  - [ ] Verify test coverage meets 80% minimum for streaming code

### **Phase 4: Documentation & Review**
- [ ] **Task 8**: Streaming Documentation & Debug Integration
  - [ ] Document frame streaming architecture and performance patterns
  - [ ] Add streaming metrics and face detection results to debug view
  - [ ] Create streaming troubleshooting guide
  - [ ] Prepare streaming service for final UI integration (Story 2.3)

- [ ] Task 1: Implement Frame Capture Service
  - [ ] Create frame capture service in core/services/
  - [ ] Implement camera frame extraction at 30 FPS
  - [ ] Handle frame capture timing and synchronization
  - [ ] Add frame capture error handling
  - [ ] Test frame capture performance

- [ ] Task 2: Frame Format Conversion
  - [ ] Implement camera frame to base64 conversion
  - [ ] Optimize conversion performance for real-time processing
  - [ ] Handle different camera formats (YUV420, etc.)
  - [ ] Add conversion error handling and validation
  - [ ] Test conversion accuracy and performance

- [ ] Task 3: Performance Optimization
  - [ ] Implement frame rate throttling if needed
  - [ ] Optimize memory usage for frame processing
  - [ ] Add frame queue management to prevent backlog
  - [ ] Implement adaptive quality based on performance
  - [ ] Monitor and optimize CPU usage

### **Phase 2: Frame Streaming Implementation (ACs: 3, 4, 8, 10)**

- [ ] Task 4: Frame Streaming Service
  - [ ] Create frame streaming service using WebSocket service
  - [ ] Implement base64 frame transmission
  - [ ] Add streaming start/stop functionality
  - [ ] Handle streaming errors and connection failures
  - [ ] Test streaming reliability and performance

- [ ] Task 5: Streaming State Management
  - [ ] Extend CheckInState to include streaming states
  - [ ] Add streaming-related events to CheckInEvent
  - [ ] Implement streaming state transitions in CheckInBloc
  - [ ] Add streaming status tracking and logging
  - [ ] Test state transitions and UI updates

- [ ] Task 6: Streaming Control Events
  - [ ] Implement start streaming event handling
  - [ ] Add stop streaming event handling
  - [ ] Handle streaming pause/resume events
  - [ ] Add streaming error event handling
  - [ ] Test streaming control flow

### **Phase 3: Response Processing & Parsing (ACs: 5, 6, 9)**

- [ ] Task 7: Response Message Parsing
  - [ ] Implement backend response parsing (JSON format)
  - [ ] Handle frameResult type and faces array structure
  - [ ] Add response validation and error handling
  - [ ] Parse face detection coordinates and confidence
  - [ ] Test parsing with various response formats

- [ ] Task 8: Face Detection Data Management
  - [ ] Create face detection data models
  - [ ] Implement face detection result storage in BLoC state
  - [ ] Add face detection history tracking
  - [ ] Handle multiple faces in response
  - [ ] Test data management and state updates

- [ ] Task 9: Response Error Handling
  - [ ] Handle malformed response messages
  - [ ] Add timeout handling for response processing
  - [ ] Implement response validation errors
  - [ ] Handle backend error responses
  - [ ] Test error scenarios and recovery

### **Phase 4: BLoC Integration & State Management (ACs: 4, 6, 7, 12)**

- [ ] Task 10: Complete BLoC State Integration
  - [ ] Extend CheckInState for frame streaming and face detection
  - [ ] Add all streaming and detection events to CheckInEvent
  - [ ] Implement complete state machine in CheckInBloc
  - [ ] Add face detection state tracking
  - [ ] Test complete state integration

- [ ] Task 11: Face Detection State Updates
  - [ ] Implement face detection result â†’ BLoC state mapping
  - [ ] Add face detection confidence tracking
  - [ ] Handle face detection status changes
  - [ ] Update UI state based on detection results
  - [ ] Test face detection state updates

- [ ] Task 12: Real-time UI Integration
  - [ ] Connect face detection results to UI updates
  - [ ] Implement real-time face detection status display
  - [ ] Add visual feedback for face detection confidence
  - [ ] Handle UI updates for multiple faces
  - [ ] Test real-time UI responsiveness

### **Phase 5: Debug Integration & Final Testing (ACs: 10, 11, 12)**

- [ ] Task 13: Debug Integration & Metrics
  - [ ] Add frame streaming metrics to debug view
  - [ ] Implement frame rate monitoring and display
  - [ ] Add processing performance metrics
  - [ ] Display face detection results in debug view
  - [ ] Test debug information accuracy

- [ ] Task 14: Final Integration & Validation
  - [ ] Integrate complete frame streaming with check-in screen
  - [ ] Test end-to-end frame streaming and processing
  - [ ] Validate real-time performance requirements
  - [ ] Ensure seamless integration with all previous stories
  - [ ] Prepare for final UI polish (Story 1.3 and 2.3)

## Dev Technical Guidance

### Frame Streaming Architecture
```dart
// Complete frame streaming service structure
class FrameStreamingService {
  static final FrameStreamingService _instance = FrameStreamingService._internal();
  factory FrameStreamingService() => _instance;
  
  Timer? _streamingTimer;
  bool _isStreaming = false;
  StreamSubscription? _cameraSubscription;
  
  // Dependencies
  final WebSocketService _webSocketService = WebSocketService();
  final CameraService _cameraService = CameraService();
  
  // Streaming control
  Future<void> startStreaming();
  Future<void> stopStreaming();
  Future<void> pauseStreaming();
  Future<void> resumeStreaming();
  
  // Frame processing
  Future<String> _convertFrameToBase64(CameraImage frame);
  void _sendFrame(String base64Frame);
  
  // State streams
  Stream<StreamingStatus> get streamingStatusStream;
  Stream<FrameMetrics> get frameMetricsStream;
  
  // Performance monitoring
  FrameMetrics get currentMetrics;
}

enum StreamingStatus {
  idle,
  starting,
  active,
  paused,
  stopping,
  error
}
```

### BLoC Integration Pattern
```dart
// Extended CheckInState for frame streaming and face detection
@freezed
class CheckInState with _$CheckInState {
  const factory CheckInState({
    @Default(CameraStatus.initial) CameraStatus cameraStatus,
    @Default(PermissionStatus.initial) PermissionStatus permissionStatus,
    @Default(ConnectionStatus.disconnected) ConnectionStatus connectionStatus,
    @Default(StreamingStatus.idle) StreamingStatus streamingStatus,
    @Default(false) bool isLoading,
    String? errorMessage,
    CameraController? cameraController,
    // WebSocket specific state
    @Default(0) int connectionAttempts,
    DateTime? lastConnectionAttempt,
    String? connectionError,
    // Frame streaming state
    @Default(0.0) double currentFrameRate,
    @Default(0) int framesProcessed,
    DateTime? lastFrameSent,
    // Face detection state
    @Default([]) List<FaceDetectionResult> detectedFaces,
    @Default(FaceDetectionStatus.none) FaceDetectionStatus faceStatus,
    @Default(0.0) double faceConfidence,
    DateTime? lastFaceDetection,
    // Toast-related state
    @Default(ToastStatus.none) ToastStatus toastStatus,
    String? toastMessage,
  }) = _CheckInState;
}

enum FaceDetectionStatus {
  none,
  detecting,
  faceFound,
  multipleFaces,
  noFace,
  error
}
```

### Frame Streaming Events Extension
```dart
// Extended CheckInEvent for frame streaming and face detection
@freezed
class CheckInEvent with _$CheckInEvent {
  // Existing events...
  
  // Frame streaming events
  const factory CheckInEvent.streamingStartRequested() = StreamingStartRequested;
  const factory CheckInEvent.streamingStarted() = StreamingStarted;
  const factory CheckInEvent.streamingPaused() = StreamingPaused;
  const factory CheckInEvent.streamingResumed() = StreamingResumed;
  const factory CheckInEvent.streamingStopped() = StreamingStopped;
  const factory CheckInEvent.streamingError(String error) = StreamingError;
  
  // Frame processing events
  const factory CheckInEvent.frameProcessed(String frameId) = FrameProcessed;
  const factory CheckInEvent.frameSent(String frameId) = FrameSent;
  const factory CheckInEvent.frameProcessingError(String error) = FrameProcessingError;
  
  // Face detection response events
  const factory CheckInEvent.faceDetectionResponse(FaceDetectionResponse response) = FaceDetectionResponseEvent;
  const factory CheckInEvent.faceDetected(List<FaceDetectionResult> faces) = FaceDetected;
  const factory CheckInEvent.noFaceDetected() = NoFaceDetected;
  const factory CheckInEvent.faceDetectionError(String error) = FaceDetectionError;
}
```

### Face Detection Data Models
```dart
// Face detection response models
@freezed
class FaceDetectionResponse with _$FaceDetectionResponse {
  const factory FaceDetectionResponse({
    required String type, // "frameResult"
    required List<FaceDetectionResult> faces,
    String? frameId,
    DateTime? timestamp,
  }) = _FaceDetectionResponse;
  
  factory FaceDetectionResponse.fromJson(Map<String, dynamic> json) =>
      _$FaceDetectionResponseFromJson(json);
}

@freezed
class FaceDetectionResult with _$FaceDetectionResult {
  const factory FaceDetectionResult({
    required double x,
    required double y,
    required double width,
    required double height,
    required double confidence,
    String? personId,
    String? personName,
  }) = _FaceDetectionResult;
  
  factory FaceDetectionResult.fromJson(Map<String, dynamic> json) =>
      _$FaceDetectionResultFromJson(json);
}
```

### Frame Processing Implementation
```dart
// Frame processing and conversion
class FrameProcessor {
  static Future<String> convertCameraImageToBase64(CameraImage image) async {
    try {
      // Convert YUV420 to RGB
      final bytes = await _convertYUV420ToRGB(image);
      
      // Encode to base64
      final base64String = base64Encode(bytes);
      
      return base64String;
    } catch (e) {
      throw FrameProcessingException('Failed to convert frame: $e');
    }
  }
  
  static Future<Uint8List> _convertYUV420ToRGB(CameraImage image) async {
    // Implementation for YUV420 to RGB conversion
    // This is a complex conversion that needs to be optimized
    // for real-time performance
  }
}
```

### Response Processing Implementation
```dart
// Response processing service
class ResponseProcessor {
  static FaceDetectionResponse? processWebSocketMessage(dynamic message) {
    try {
      if (message is String) {
        final json = jsonDecode(message);
        if (json['type'] == 'frameResult') {
          return FaceDetectionResponse.fromJson(json);
        }
      }
      return null;
    } catch (e) {
      throw ResponseProcessingException('Failed to process response: $e');
    }
  }
  
  static FaceDetectionStatus determineFaceStatus(List<FaceDetectionResult> faces) {
    if (faces.isEmpty) {
      return FaceDetectionStatus.noFace;
    } else if (faces.length == 1) {
      return faces.first.confidence > 0.7 
          ? FaceDetectionStatus.faceFound 
          : FaceDetectionStatus.detecting;
    } else {
      return FaceDetectionStatus.multipleFaces;
    }
  }
}
```

### Performance Specifications
- **Frame Rate**: 30 FPS target (adjustable based on performance)
- **Frame Resolution**: 640x480 (optimized for recognition and bandwidth)
- **Conversion Time**: <33ms per frame (to maintain 30 FPS)
- **Network Bandwidth**: Approximately 1-2 MB/s for base64 streaming
- **Memory Usage**: Efficient frame processing without memory leaks
- **CPU Usage**: <50% CPU for frame processing on mid-range devices

### Real-time Processing Requirements
- **Frame Processing**: Real-time conversion and streaming
- **State Updates**: Immediate UI updates on face detection
- **Response Time**: <100ms from frame capture to state update
- **Error Recovery**: Graceful handling without streaming interruption
- **Performance Monitoring**: Real-time metrics for debugging

### Backend Response Format
```json
{
  "type": "frameResult",
  "faces": [
    {
      "x": 100,
      "y": 150,
      "width": 200,
      "height": 250,
      "confidence": 0.95,
      "personId": "optional",
      "personName": "optional"
    }
  ],
  "frameId": "optional",
  "timestamp": "2024-01-01T10:00:00Z"
}
```

### Error Handling Strategy
- **Frame Processing Errors**: Retry with fallback quality
- **Network Errors**: Buffer frames and retry transmission
- **Response Parsing Errors**: Log and continue processing
- **Performance Issues**: Adaptive frame rate and quality
- **Memory Issues**: Garbage collection and resource cleanup

### Performance Optimization Strategy
- **Frame Buffering**: Smart buffering to prevent memory issues
- **Adaptive Quality**: Reduce quality if performance degrades
- **Batch Processing**: Process multiple frames efficiently
- **Memory Management**: Efficient cleanup of processed frames
- **Network Optimization**: Compress frames if needed for bandwidth

### **TDD-Specific Requirements**
- **Test Coverage**: Minimum 80% for frame processing, streaming service, and response handling
- **Test Quality**: Real-time tests should handle timing, performance, and concurrency properly
- **Test Organization**: Separate performance tests, unit tests for logic, widget tests for UI
- **Mock Strategy**: Use controlled frame mocking and timing simulation for reliable testing

### **Frame Streaming TDD Testing Patterns**
```dart
// Example TDD approach for frame streaming testing
group('FrameStreamingService', () {
  late FrameStreamingService streamingService;
  late MockWebSocketService mockWebSocketService;
  late MockCameraService mockCameraService;

  setUp(() {
    mockWebSocketService = MockWebSocketService();
    mockCameraService = MockCameraService();
    streamingService = FrameStreamingService(
      webSocketService: mockWebSocketService,
      cameraService: mockCameraService,
    );
  });

  // RED: Write failing test first
  test('should capture frames at 30 FPS when streaming starts', () async {
    // arrange
    final mockFrames = List.generate(30, (i) => MockCameraImage());
    when(() => mockCameraService.frameStream)
        .thenAnswer((_) => Stream.periodic(
          Duration(milliseconds: 33), // 30 FPS
          (i) => mockFrames[i % mockFrames.length],
        ));

    // act
    await streamingService.startStreaming();
    await Future.delayed(Duration(seconds: 1));

    // assert
    expect(streamingService.currentFrameRate, closeTo(30.0, 2.0));
  });

  // Test base64 conversion performance
  test('should convert frame to base64 within 33ms', () async {
    // arrange
    final mockFrame = MockCameraImage();
    
    // act
    final stopwatch = Stopwatch()..start();
    final base64Frame = await FrameProcessor.convertCameraImageToBase64(mockFrame);
    stopwatch.stop();
    
    // assert
    expect(stopwatch.elapsedMilliseconds, lessThan(33));
    expect(base64Frame, isNotEmpty);
  });
});

// Response processing tests
group('ResponseProcessor', () {
  test('should parse face detection response correctly', () {
    // arrange
    const responseJson = '''
    {
      "type": "frameResult",
      "faces": [
        {
          "x": 100,
          "y": 150,
          "width": 200,
          "height": 250,
          "confidence": 0.95
        }
      ]
    }
    ''';

    // act
    final response = ResponseProcessor.processWebSocketMessage(responseJson);

    // assert
    expect(response, isNotNull);
    expect(response!.faces.length, equals(1));
    expect(response.faces.first.confidence, equals(0.95));
  });
});

// Real-time UI tests
group('Real-time Face Detection UI', () {
  testWidgets('should update UI when face detection results change', (tester) async {
    // arrange
    final mockBloc = MockCheckInBloc();
    const initialState = CheckInState();
    const faceDetectedState = CheckInState(
      faceStatus: FaceDetectionStatus.faceFound,
      faceConfidence: 0.95,
    );

    whenListen(mockBloc, Stream.fromIterable([initialState, faceDetectedState]));

    // act
    await tester.pumpWidget(
      MaterialApp(
        home: BlocProvider.value(
          value: mockBloc,
          child: CheckInScreen(),
        ),
      ),
    );

    await tester.pump(); // Initial state
    await tester.pump(); // Face detected state

    // assert
    expect(find.text('Face Detected'), findsOneWidget);
  });
});
```

### **Quality Standards**
- **Real-time Performance**: 30 FPS frame processing with <33ms conversion time
- **Memory Management**: Efficient frame processing without memory leaks
- **Network Reliability**: Robust streaming with error recovery and retry logic
- **UI Responsiveness**: Real-time UI updates without blocking main thread
- **Testing**: Complete TDD coverage with performance and timing tests

## Definition of Done Checklist

### **TDD-Specific DoD**
- [ ] **Test-First Development**: All frame streaming features developed using TDD RED-GREEN-REFACTOR cycle
- [ ] **Test Coverage**: 80%+ coverage achieved for frame processing, streaming service, and response handling
- [ ] **Test Quality**: Real-time tests handle timing, performance, and concurrency scenarios properly
- [ ] **Test Documentation**: Frame streaming testing strategy and performance benchmarks documented
- [ ] **Mock Strategy**: Frame processing and WebSocket responses properly mocked for reliable testing

### **Standard DoD**
- [ ] **Code Review**: Reviewed by team member with real-time processing and TDD experience
- [ ] **Integration**: Frame streaming seamlessly integrated with camera and WebSocket services
- [ ] **Documentation**: Frame streaming architecture and performance patterns documented
- [ ] **Performance Testing**: Real-time performance verified on multiple device types
- [ ] **Memory Testing**: Memory usage optimized and leak-free operation verified

### **Story-Specific Requirements**
- [ ] **Frame Processing**: 30 FPS frame capture and base64 conversion working efficiently
- [ ] **Streaming Integration**: Base64 frames streaming to WebSocket backend successfully
- [ ] **Response Processing**: Face detection responses parsed and processed correctly
- [ ] **Real-time UI**: UI updates in real-time based on face detection results
- [ ] **Performance**: Real-time processing requirements met (<33ms per frame)
- [ ] **Error Handling**: Comprehensive error handling for all streaming scenarios
- [ ] **Debug Integration**: Streaming metrics and face detection results visible in debug view

### **Performance Requirements**
- **Frame Rate**: Consistent 30 FPS frame processing
- **Conversion Time**: <33ms per frame for base64 conversion
- **Memory Usage**: Efficient processing without memory leaks
- **CPU Usage**: <50% CPU usage on mid-range devices
- **Network Bandwidth**: Optimized streaming (1-2 MB/s)
- **Response Time**: <100ms from detection to UI update

### **Integration Points**
- **Camera Service**: Frame capture integration with camera stream
- **WebSocket Service**: Frame transmission and response handling
- **BLoC State**: Real-time state updates for UI components
- **Debug View**: Performance metrics and detection results display
- **UI Components**: Real-time face detection status and visual feedback

## Story Progress Notes

### Agent Model Used: `<To be filled by implementing agent>`

### Completion Notes List

*{Implementation notes will be filled by the implementing agent}*

### Change Log 