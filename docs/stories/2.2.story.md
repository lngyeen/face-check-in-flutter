---
description: Story for implementing frame streaming, processing, and UI updates
---

# Story 2.2: Frame Streaming & Processing

## Status: Ready for Development

## Story

- As a **user**
- I want **the app to capture camera frames and stream them to the backend for face recognition, then process the responses**
- so that **I can see real-time face detection results and status updates**

## Acceptance Criteria (ACs)

1.  Camera frames are captured at 30 FPS from the camera stream.
2.  Frames are converted to base64 format for transmission.
3.  Base64 frames are sent to the backend WebSocket connection.
4.  Frame streaming state is managed through BLoC with proper state tracking.
5.  Backend response messages are received and parsed correctly.
6.  Face detection results are processed and stored in BLoC state.
7.  UI updates in real-time based on face detection results.
8.  Frame streaming can be started/stopped through BLoC events.
9.  Error handling for frame processing and streaming failures.
10. Frame streaming metrics are logged to the debug view.
11. Processing performance is optimized for real-time operation.
12. Face detection status is displayed to the user through UI.
13. **Crucially**, the entire feature is validated through integration testing on a physical device.

## Story Estimation

**Story Points**: 18 SP
**Complexity**: Very High - Real-time frame processing + Network streaming + Response handling + BLoC integration + Performance optimization
**Risk Level**: High - Performance requirements, real-time processing, network bandwidth, state synchronization
**Assumptions**: Camera service ready (Story 1.2), WebSocket service ready (Story 2.1), backend response format documented

## Development Plan: Tasks & Commits

---

### **Phase 1: Frame Capture & Conversion (ACs: 1, 2, 11)**

**Phase Status**: `Ready for Development`

-   **Sub-task 1.1: Setup Frame Capture Service**
    -   [ ] Create `frame_capture_service.dart` in `lib/core/services/`.
    -   **Commit:** `feat(camera): setup initial frame capture service structure`
-   **Sub-task 1.2: Implement Frame Extraction**
    -   [ ] Integrate with `camera_service` to get `CameraImage` stream.
    -   [ ] Implement logic to extract frames at a target rate (e.g., 30 FPS).
    -   [ ] Add basic error handling for stream subscription.
    -   **Commit:** `feat(camera): implement frame extraction from camera stream`
-   **Sub-task 1.3: Implement Frame to Base64 Conversion**
    -   [ ] Create a utility function to convert `CameraImage` (YUV) to a more common format like JPG.
    -   [ ] Convert the JPG image bytes to a Base64 `String`.
    -   [ ] Add error handling for the conversion process.
    -   **Commit:** `feat(camera): implement frame to base64 conversion`
-   **Sub-task 1.4: Performance Tuning & Optimization**
    -   [ ] Analyze and optimize the conversion process for real-time performance.
    -   [ ] Implement frame throttling if CPU usage is too high.
    -   [ ] Manage memory to avoid leaks from frame processing.
    -   **Commit:** `perf(camera): optimize frame capture and conversion performance`
-   **Testing & Validation (Phase 1):**
    -   [ ] **Unit Test:** Write unit tests for the `frame_capture_service` and conversion utilities.
    -   [ ] **Widget Test:** Create a simple widget test to validate service integration.
    -   [ ] **Manual Test:** Visually confirm frame capture and log the Base64 output to ensure correctness.
-   **Summary & PR (Phase 1):**
    -   After all sub-tasks and testing are complete, create a Pull Request for `Phase 1`.
    -   Once the PR is merged, update **Phase Status** to `Review`, then `Done`.

---

### **Phase 2: Frame Streaming & BLoC Integration (ACs: 3, 4, 8)**

**Phase Status**: `Ready for Development`

-   **Sub-task 2.1: Setup Frame Streaming Service**
    -   [ ] Create `frame_streaming_service.dart` that uses the `WebSocketService`.
    -   [ ] Implement `startStreaming` and `stopStreaming` methods.
    -   **Commit:** `feat(streaming): setup initial frame streaming service`
-   **Sub-task 2.2: Integrate Frame Capture with Streaming**
    -   [ ] Connect `frame_capture_service` output to `frame_streaming_service`.
    -   [ ] Send Base64 frames via the WebSocket connection when streaming is active.
    -   [ ] Add error handling for the WebSocket `send` operation.
    -   **Commit:** `feat(streaming): connect frame capture and send frames via websocket`
-   **Sub-task 2.3: Extend CheckIn BLoC for Streaming**
    -   [ ] Add `StreamingStatus` enum (`idle`, `active`, `error`).
    -   [ ] Update `CheckInState` to include `streamingStatus`.
    -   [ ] Add `StreamingStartRequested` and `StreamingStopRequested` to `CheckInEvent`.
    -   **Commit:** `feat(bloc): extend check_in_bloc for frame streaming state`
-   **Sub-task 2.4: Implement Streaming Logic in BLoC**
    -   [ ] Handle `StreamingStartRequested` to call `frame_streaming_service.startStreaming`.
    -   [ ] Handle `StreamingStopRequested` to call `frame_streaming_service.stopStreaming`.
    -   [ ] Listen to status updates from the service and emit new `CheckInState`.
    -   **Commit:** `feat(bloc): implement streaming start/stop logic in check_in_bloc`
-   **Testing & Validation (Phase 2):**
    -   [ ] **Unit Test:** Write unit tests for `frame_streaming_service` and BLoC streaming logic.
    -   [ ] **Integration Test:** Test the flow from BLoC event -> Streaming Service -> WebSocket.
    -   [ ] **Manual Test:** Use the UI to start/stop streaming and verify frames are sent via debug logs/network inspector.
-   **Summary & PR (Phase 2):**
    -   Create a Pull Request for `Phase 2`.
    -   Once merged, update **Phase Status** to `Review`, then `Done`.

---

### **Phase 3: Backend Response Processing (ACs: 5, 6, 9)**

**Phase Status**: `Ready for Development`

-   **Sub-task 3.1: Define Response Data Models**
    -   [ ] Create data models (e.g., `FaceDetectionResult`) for the backend's JSON response.
    -   [ ] Include properties like `faces`, `coordinates`, `confidence`.
    -   **Commit:** `feat(model): define data models for face detection response`
-   **Sub-task 3.2: Implement Response Parsing**
    -   [ ] In `WebSocketService`, listen for incoming messages.
    -   [ ] Parse the JSON string into the `FaceDetectionResult` model.
    -   [ ] Add robust error handling for malformed JSON.
    -   **Commit:** `feat(streaming): implement parsing for backend websocket responses`
-   **Sub-task 3.3: Extend BLoC for Face Detection Data**
    -   [ ] Add `detectedFaces` and `faceStatus` to `CheckInState`.
    -   [ ] Create `BackendResponseReceived` event in `CheckInEvent` with the parsed data.
    -   **Commit:** `feat(bloc): extend check_in_bloc state for face detection results`
-   **Sub-task 3.4: Integrate Response into BLoC**
    -   [ ] `WebSocketService` adds a `BackendResponseReceived` event to the BLoC.
    -   [ ] Handle the event in `CheckInBloc` to update the state with `detectedFaces` and `faceStatus`.
    -   **Commit:** `feat(bloc): integrate backend response processing into check_in_bloc`
-   **Testing & Validation (Phase 3):**
    -   [ ] **Unit Test:** Test the JSON parsing logic with valid and invalid data. Test BLoC state updates.
    -   [ ] **Integration Test:** Mock the WebSocket server to send various responses and verify the BLoC state changes accordingly.
-   **Summary & PR (Phase 3):**
    -   Create a Pull Request for `Phase 3`.
    -   Once merged, update **Phase Status** to `Review`, then `Done`.

---

### **Phase 4: UI Integration & Real-time Updates (ACs: 7, 10, 12)**

**Phase Status**: `Ready for Development`

-   **Sub-task 4.1: Display Face Detection Status**
    -   [ ] Create a new widget to display the current `faceStatus` (e.g., "Face Found", "No Face").
    -   [ ] Use a `BlocBuilder` to listen to `CheckInState` and update the widget.
    -   **Commit:** `feat(ui): display real-time face detection status`
-   **Sub-task 4.2: Visualize Face Bounding Boxes**
    -   [ ] Create a `CustomPainter` to draw rectangles over the camera preview.
    -   [ ] Use `detectedFaces` from `CheckInState` to get the coordinates for the bounding boxes.
    -   **Commit:** `feat(ui): draw bounding boxes for detected faces on camera preview`
-   **Sub-task 4.3: Update Debug View**
    -   [ ] Add streaming metrics (FPS, frames sent) to the debug view.
    -   [ ] Display raw face detection data (confidence, etc.) in the debug view.
    -   **Commit:** `feat(debug): add frame streaming and detection metrics to debug view`
-   **Testing & Validation (Phase 4):**
    -   [ ] **Widget Test:** Write widget tests for the new status and bounding box widgets.
    -   [ ] **Manual Test:** Visually verify that the UI updates correctly based on mocked backend responses.
-   **Summary & PR (Phase 4):**
    -   Create a Pull Request for `Phase 4`.
    -   Once merged, update **Phase Status** to `Review`, then `Done`.

---

### **Phase 5: Final Integration & Validation (ACs: 9, 11, 13)**

**Phase Status**: `Ready for Development`

-   **Sub-task 5.1: End-to-End Review**
    -   [ ] Review the entire workflow from camera -> streaming -> backend -> UI.
    -   [ ] Ensure all error handling paths are covered.
    -   **Commit:** `refactor: review and polish end-to-end frame streaming flow`
-   **Sub-task 5.2: Real Device Integration Testing**
    -   [ ] Deploy the application to a physical Android device.
    -   [ ] Deploy the application to a physical iOS device.
    -   [ ] **CONFIRMATION:** Perform comprehensive testing of the entire feature on both platforms.
        -   [ ] Verify camera performance.
        -   [ ] Verify streaming stability and network usage.
        -   [ ] Verify UI responsiveness and correctness of face detection display.
    -   **Commit:** `test: perform and validate integration on real devices`
-   **Summary & PR (Phase 5):**
    -   Create a final Pull Request for `Phase 5`.
    -   Once merged, this story is considered complete. Update main story **Status** to `Review`, then `Done`.

## Dev Technical Guidance

### Frame Streaming Architecture
```dart
// Complete frame streaming service structure
class FrameStreamingService {
  static final FrameStreamingService _instance = FrameStreamingService._internal();
  factory FrameStreamingService() => _instance;
  
  Timer? _streamingTimer;
  bool _isStreaming = false;
  StreamSubscription? _cameraSubscription;
  
  // Dependencies
  final WebSocketService _webSocketService = WebSocketService();
  final CameraService _cameraService = CameraService();
  
  // Streaming control
  Future<void> startStreaming();
  Future<void> stopStreaming();
  Future<void> pauseStreaming();
  Future<void> resumeStreaming();
  
  // Frame processing
  Future<String> _convertFrameToBase64(CameraImage frame);
  void _sendFrame(String base64Frame);
  
  // State streams
  Stream<StreamingStatus> get streamingStatusStream;
  Stream<FrameMetrics> get frameMetricsStream;
  
  // Performance monitoring
  FrameMetrics get currentMetrics;
}

enum StreamingStatus {
  idle,
  starting,
  active,
  paused,
  stopping,
  error
}
```

### BLoC Integration Pattern
```dart
// Extended CheckInState for frame streaming and face detection
@freezed
class CheckInState with _$CheckInState {
  const factory CheckInState({
    @Default(CameraStatus.initial) CameraStatus cameraStatus,
    @Default(PermissionStatus.initial) PermissionStatus permissionStatus,
    @Default(ConnectionStatus.disconnected) ConnectionStatus connectionStatus,
    @Default(StreamingStatus.idle) StreamingStatus streamingStatus,
    @Default(false) bool isLoading,
    String? errorMessage,
    CameraController? cameraController,
    // WebSocket specific state
    @Default(0) int connectionAttempts,
    DateTime? lastConnectionAttempt,
    String? connectionError,
    // Frame streaming state
    @Default(0.0) double currentFrameRate,
    @Default(0) int framesProcessed,
    DateTime? lastFrameSent,
    // Face detection state
    @Default([]) List<FaceDetectionResult> detectedFaces,
    @Default(FaceDetectionStatus.none) FaceDetectionStatus faceStatus,
    @Default(0.0) double faceConfidence,
    DateTime? lastFaceDetection,
    // Toast-related state
    @Default(ToastStatus.none) ToastStatus toastStatus,
    String? toastMessage,
  }) = _CheckInState;
}

enum FaceDetectionStatus {
  none,
  detecting,
  faceFound,
  multipleFaces,
  noFace,
  error
}
```

### Frame Streaming Events Extension
```dart
// Extended CheckInEvent for frame streaming and face detection
@freezed
class CheckInEvent with _$CheckInEvent {
  // Existing events...
  
  // Frame streaming events
  const factory CheckInEvent.streamingStartRequested() = StreamingStartRequested;
  const factory CheckInEvent.streamingStarted() = StreamingStarted;
  const factory CheckInEvent.streamingPaused() = StreamingPaused;
  const factory CheckInEvent.streamingResumed() = StreamingResumed;
  const factory CheckInEvent.streamingStopped() = StreamingStopped;
  const factory CheckInEvent.streamingError(String error) = StreamingError;
  
  // Frame processing events
  const factory CheckInEvent.frameProcessed(String frameId) = FrameProcessed;
  const factory CheckInEvent.frameSent(String frameId) = FrameSent;
  const factory CheckInEvent.frameProcessingError(String error) = FrameProcessingError;
  
  // Backend response events
  const factory CheckInEvent.backendResponseReceived(FaceDetectionResult result) = BackendResponseReceived;
}
```

### Backend Response Format (Expected)
```json
{
  "type": "frameResult",
  "data": {
    "frameId": "some-uuid-1234",
    "timestamp": "2023-10-27T10:00:00Z",
    "faces": [
      {
        "faceId": "face-uuid-5678",
        "box": [100, 150, 200, 250],
        "confidence": 0.98,
        "isRecognized": false,
        "personId": null
      }
    ],
    "status": "face_found"
  }
}
```

## Story Progress Notes

### Agent Model Used: `<To be filled by implementing agent>`

### Completion Notes List

*{Implementation notes will be filled by the implementing agent}*

### Change Log 