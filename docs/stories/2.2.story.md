# Story 2.2: Frame Streaming & Processing

## Status: InProgress (Phase 2 âœ… Complete, Phase 3 ðŸ”„ Next)

## Story

- As a **user**
- I want **the app to capture camera frames and stream them to the backend for face recognition, then process the responses**
- so that **I can see real-time face detection results and status updates**

## Acceptance Criteria (ACs)

1. Camera frames are captured at 30 FPS from the camera stream
2. Frames are converted to base64 format for transmission
3. Base64 frames are sent to backend WebSocket connection
4. Frame streaming state is managed through BLoC with proper state tracking
5. Backend response messages are received and parsed correctly
6. Face detection results are processed and stored in BLoC state
7. UI updates in real-time based on face detection results
8. Frame streaming can be started/stopped through BLoC events
9. Error handling for frame processing and streaming failures
10. Frame streaming metrics are logged to debug view
11. Processing performance is optimized for real-time operation
12. Face detection status is displayed to the user through UI

## Story Estimation

**Story Points**: 18 SP  
**Complexity**: Very High - Real-time frame processing + Network streaming + Response handling + BLoC integration + Performance optimization  
**Risk Level**: High - Performance requirements, real-time processing, network bandwidth, state synchronization  
**Estimation Method**: Combined estimation from frame streaming and response processing stories  
**Reference Stories**: Similar real-time streaming implementations with state management  
**Assumptions**: Camera service ready (Story 1.2), WebSocket service ready (Story 2.1), backend response format documented

## Tasks / Subtasks

### **Phase 1: Frame Capture & Conversion (ACs: 1, 2, 11)** âœ…

- [x] Task 1: Implement Frame Capture Service
  - [x] Create frame capture service in core/services/
  - [x] Implement camera frame extraction at 30 FPS
  - [x] Handle frame capture timing and synchronization
  - [x] Add frame capture error handling
  - [x] Test frame capture performance

- [x] Task 2: Frame Format Conversion
  - [x] Implement camera frame to base64 conversion
  - [x] Optimize conversion performance for real-time processing
  - [x] Handle different camera formats (YUV420, etc.)
  - [x] Add conversion error handling and validation
  - [x] Test conversion accuracy and performance

- [x] Task 3: Performance Optimization
  - [x] Implement frame rate throttling if needed
  - [x] Optimize memory usage for frame processing
  - [x] Add frame queue management to prevent backlog
  - [x] Implement adaptive quality based on performance
  - [x] Monitor and optimize CPU usage

### **Phase 2: Frame Streaming Implementation (ACs: 3, 4, 8, 10)** âœ… **COMPLETED**

- [x] Task 4: Frame Streaming Service
  - [x] Create frame streaming service using WebSocket service
  - [x] Implement base64 frame transmission
  - [x] Add streaming start/stop functionality
  - [x] Handle streaming errors and connection failures
  - [x] Test streaming reliability and performance

- [x] Task 5: Streaming State Management
  - [x] Extend CheckInState to include streaming states
  - [x] Add streaming-related events to CheckInEvent
  - [x] Implement streaming state transitions in CheckInBloc
  - [x] Add streaming status tracking and logging
  - [x] Test state transitions and UI updates

- [x] Task 6: Streaming Control Events
  - [x] Implement start streaming event handling
  - [x] Add stop streaming event handling
  - [x] Handle streaming pause/resume events
  - [x] Add streaming error event handling
  - [x] Test streaming control flow

**Phase 2 Implementation Summary:**
- âœ… **FrameStreamingService**: Complete WebSocket-based frame transmission with 30 FPS support
- âœ… **Performance Metrics**: Real-time FPS, latency, throughput, and success rate tracking
- âœ… **State Management**: Extended CheckInState with 13+ new streaming & face detection fields
- âœ… **Event Handling**: 26 comprehensive events for streaming control and face detection
- âœ… **Error Recovery**: Robust auto-reconnection and graceful degradation
- âœ… **BLoC Integration**: Complete integration with dependency injection
- âœ… **Face Detection Ready**: Models and state management prepared for Phase 3
- âœ… **Code Quality**: 14,989 lines added, all tests passing, analyzer clean
- âœ… **PR Created**: [PR #15](https://github.com/lngyeen/face-check-in-flutter/pull/15) - Ready for review

### **Phase 3: Response Processing & Parsing (ACs: 5, 6, 9)**

- [ ] Task 7: Response Message Parsing
  - [ ] Implement backend response parsing (JSON format)
  - [ ] Handle frameResult type and faces array structure
  - [ ] Add response validation and error handling
  - [ ] Parse face detection coordinates and confidence
  - [ ] Test parsing with various response formats

- [ ] Task 8: Face Detection Data Management
  - [ ] Create face detection data models
  - [ ] Implement face detection result storage in BLoC state
  - [ ] Add face detection history tracking
  - [ ] Handle multiple faces in response
  - [ ] Test data management and state updates

- [ ] Task 9: Response Error Handling
  - [ ] Handle malformed response messages
  - [ ] Add timeout handling for response processing
  - [ ] Implement response validation errors
  - [ ] Handle backend error responses
  - [ ] Test error scenarios and recovery

### **Phase 4: BLoC Integration & State Management (ACs: 4, 6, 7, 12)**

- [ ] Task 10: Complete BLoC State Integration
  - [ ] Extend CheckInState for frame streaming and face detection
  - [ ] Add all streaming and detection events to CheckInEvent
  - [ ] Implement complete state machine in CheckInBloc
  - [ ] Add face detection state tracking
  - [ ] Test complete state integration

- [ ] Task 11: Face Detection State Updates
  - [ ] Implement face detection result â†’ BLoC state mapping
  - [ ] Add face detection confidence tracking
  - [ ] Handle face detection status changes
  - [ ] Update UI state based on detection results
  - [ ] Test face detection state updates

- [ ] Task 12: Real-time UI Integration
  - [ ] Connect face detection results to UI updates
  - [ ] Implement real-time face detection status display
  - [ ] Add visual feedback for face detection confidence
  - [ ] Handle UI updates for multiple faces
  - [ ] Test real-time UI responsiveness

### **Phase 5: Debug Integration & Final Testing (ACs: 10, 11, 12)**

- [ ] Task 13: Debug Integration & Metrics
  - [ ] Add frame streaming metrics to debug view
  - [ ] Implement frame rate monitoring and display
  - [ ] Add processing performance metrics
  - [ ] Display face detection results in debug view
  - [ ] Test debug information accuracy

- [ ] Task 14: Final Integration & Validation
  - [ ] Integrate complete frame streaming with check-in screen
  - [ ] Test end-to-end frame streaming and processing
  - [ ] Validate real-time performance requirements
  - [ ] Ensure seamless integration with all previous stories
  - [ ] Prepare for final UI polish (Story 1.3 and 2.3)

## Dev Technical Guidance

### Frame Streaming Architecture
```dart
// Complete frame streaming service structure
class FrameStreamingService {
  static final FrameStreamingService _instance = FrameStreamingService._internal();
  factory FrameStreamingService() => _instance;
  
  Timer? _streamingTimer;
  bool _isStreaming = false;
  StreamSubscription? _cameraSubscription;
  
  // Dependencies
  final WebSocketService _webSocketService = WebSocketService();
  final CameraService _cameraService = CameraService();
  
  // Streaming control
  Future<void> startStreaming();
  Future<void> stopStreaming();
  Future<void> pauseStreaming();
  Future<void> resumeStreaming();
  
  // Frame processing
  Future<String> _convertFrameToBase64(CameraImage frame);
  void _sendFrame(String base64Frame);
  
  // State streams
  Stream<StreamingStatus> get streamingStatusStream;
  Stream<FrameMetrics> get frameMetricsStream;
  
  // Performance monitoring
  FrameMetrics get currentMetrics;
}

enum StreamingStatus {
  idle,
  starting,
  active,
  paused,
  stopping,
  error
}
```

### BLoC Integration Pattern
```dart
// Extended CheckInState for frame streaming and face detection
@freezed
class CheckInState with _$CheckInState {
  const factory CheckInState({
    @Default(CameraStatus.initial) CameraStatus cameraStatus,
    @Default(PermissionStatus.initial) PermissionStatus permissionStatus,
    @Default(ConnectionStatus.disconnected) ConnectionStatus connectionStatus,
    @Default(StreamingStatus.idle) StreamingStatus streamingStatus,
    @Default(false) bool isLoading,
    String? errorMessage,
    CameraController? cameraController,
    // WebSocket specific state
    @Default(0) int connectionAttempts,
    DateTime? lastConnectionAttempt,
    String? connectionError,
    // Frame streaming state
    @Default(0.0) double currentFrameRate,
    @Default(0) int framesProcessed,
    DateTime? lastFrameSent,
    // Face detection state
    @Default([]) List<FaceDetectionResult> detectedFaces,
    @Default(FaceDetectionStatus.none) FaceDetectionStatus faceStatus,
    @Default(0.0) double faceConfidence,
    DateTime? lastFaceDetection,
    // Toast-related state
    @Default(ToastStatus.none) ToastStatus toastStatus,
    String? toastMessage,
  }) = _CheckInState;
}

enum FaceDetectionStatus {
  none,
  detecting,
  faceFound,
  multipleFaces,
  noFace,
  error
}
```

### Frame Streaming Events Extension
```dart
// Extended CheckInEvent for frame streaming and face detection
@freezed
class CheckInEvent with _$CheckInEvent {
  // Existing events...
  
  // Frame streaming events
  const factory CheckInEvent.streamingStartRequested() = StreamingStartRequested;
  const factory CheckInEvent.streamingStarted() = StreamingStarted;
  const factory CheckInEvent.streamingPaused() = StreamingPaused;
  const factory CheckInEvent.streamingResumed() = StreamingResumed;
  const factory CheckInEvent.streamingStopped() = StreamingStopped;
  const factory CheckInEvent.streamingError(String error) = StreamingError;
  
  // Frame processing events
  const factory CheckInEvent.frameProcessed(String frameId) = FrameProcessed;
  const factory CheckInEvent.frameSent(String frameId) = FrameSent;
  const factory CheckInEvent.frameProcessingError(String error) = FrameProcessingError;
  
  // Face detection response events
  const factory CheckInEvent.faceDetectionResponse(FaceDetectionResponse response) = FaceDetectionResponseEvent;
  const factory CheckInEvent.faceDetected(List<FaceDetectionResult> faces) = FaceDetected;
  const factory CheckInEvent.noFaceDetected() = NoFaceDetected;
  const factory CheckInEvent.faceDetectionError(String error) = FaceDetectionError;
}
```

### Face Detection Data Models
```dart
// Face detection response models
@freezed
class FaceDetectionResponse with _$FaceDetectionResponse {
  const factory FaceDetectionResponse({
    required String type, // "frameResult"
    required List<FaceDetectionResult> faces,
    String? frameId,
    DateTime? timestamp,
  }) = _FaceDetectionResponse;
  
  factory FaceDetectionResponse.fromJson(Map<String, dynamic> json) =>
      _$FaceDetectionResponseFromJson(json);
}

@freezed
class FaceDetectionResult with _$FaceDetectionResult {
  const factory FaceDetectionResult({
    required double x,
    required double y,
    required double width,
    required double height,
    required double confidence,
    String? personId,
    String? personName,
  }) = _FaceDetectionResult;
  
  factory FaceDetectionResult.fromJson(Map<String, dynamic> json) =>
      _$FaceDetectionResultFromJson(json);
}
```

### Frame Processing Implementation
```dart
// Frame processing and conversion
class FrameProcessor {
  static Future<String> convertCameraImageToBase64(CameraImage image) async {
    try {
      // Convert YUV420 to RGB
      final bytes = await _convertYUV420ToRGB(image);
      
      // Encode to base64
      final base64String = base64Encode(bytes);
      
      return base64String;
    } catch (e) {
      throw FrameProcessingException('Failed to convert frame: $e');
    }
  }
  
  static Future<Uint8List> _convertYUV420ToRGB(CameraImage image) async {
    // Implementation for YUV420 to RGB conversion
    // This is a complex conversion that needs to be optimized
    // for real-time performance
  }
}
```

### Response Processing Implementation
```dart
// Response processing service
class ResponseProcessor {
  static FaceDetectionResponse? processWebSocketMessage(dynamic message) {
    try {
      if (message is String) {
        final json = jsonDecode(message);
        if (json['type'] == 'frameResult') {
          return FaceDetectionResponse.fromJson(json);
        }
      }
      return null;
    } catch (e) {
      throw ResponseProcessingException('Failed to process response: $e');
    }
  }
  
  static FaceDetectionStatus determineFaceStatus(List<FaceDetectionResult> faces) {
    if (faces.isEmpty) {
      return FaceDetectionStatus.noFace;
    } else if (faces.length == 1) {
      return faces.first.confidence > 0.7 
          ? FaceDetectionStatus.faceFound 
          : FaceDetectionStatus.detecting;
    } else {
      return FaceDetectionStatus.multipleFaces;
    }
  }
}
```

### Performance Specifications
- **Frame Rate**: 30 FPS target (adjustable based on performance)
- **Frame Resolution**: 640x480 (optimized for recognition and bandwidth)
- **Conversion Time**: <33ms per frame (to maintain 30 FPS)
- **Network Bandwidth**: Approximately 1-2 MB/s for base64 streaming
- **Memory Usage**: Efficient frame processing without memory leaks
- **CPU Usage**: <50% CPU for frame processing on mid-range devices

### Real-time Processing Requirements
- **Frame Processing**: Real-time conversion and streaming
- **State Updates**: Immediate UI updates on face detection
- **Response Time**: <100ms from frame capture to state update
- **Error Recovery**: Graceful handling without streaming interruption
- **Performance Monitoring**: Real-time metrics for debugging

### Backend Response Format
```json
{
  "type": "frameResult",
  "faces": [
    {
      "x": 100,
      "y": 150,
      "width": 200,
      "height": 250,
      "confidence": 0.95,
      "personId": "optional",
      "personName": "optional"
    }
  ],
  "frameId": "optional",
  "timestamp": "2024-01-01T10:00:00Z"
}
```

### Error Handling Strategy
- **Frame Processing Errors**: Retry with fallback quality
- **Network Errors**: Buffer frames and retry transmission
- **Response Parsing Errors**: Log and continue processing
- **Performance Issues**: Adaptive frame rate and quality
- **Memory Issues**: Garbage collection and resource cleanup

### Performance Optimization Strategy
- **Frame Buffering**: Smart buffering to prevent memory issues
- **Adaptive Quality**: Reduce quality if performance degrades
- **Batch Processing**: Process multiple frames efficiently
- **Memory Management**: Efficient cleanup of processed frames
- **Network Optimization**: Compress frames if needed for bandwidth

### Integration Points
- **Camera Service**: Frame capture integration
- **WebSocket Service**: Frame transmission and response receiving
- **BLoC State**: Complete state management for streaming and detection
- **UI Components**: Real-time face detection display
- **Debug View**: Streaming and detection metrics

### Quality Standards
- **Real-time Performance**: Maintain 30 FPS with <100ms latency
- **Accuracy**: Reliable face detection state management
- **Error Recovery**: Graceful handling of all error conditions
- **Resource Efficiency**: No memory leaks or excessive CPU usage
- **User Experience**: Smooth real-time feedback without lag

## Story Progress Notes

### Agent Model Used: `<To be filled by implementing agent>`

### Completion Notes List

*{Implementation notes will be filled by the implementing agent}*

### Change Log 